{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "084a5836-46c8-47ac-a1e0-e8348a525150",
   "metadata": {},
   "source": [
    "# ProgrammingAssignment06_clusterAnalysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bf8149-9aa3-4627-8f2c-49e531f9c5e9",
   "metadata": {},
   "source": [
    "## 1. k-means using scikit-learn\n",
    "The healthy_lifestyle dataset contains information on lifestyle measures such as amount of sunshine, pollution, and happiness levels for 44 major cities around the world. Apply k-means clustering to the cities' number of hours of sunshine and happiness levels.\n",
    "\n",
    "- Import the needed packages for clustering.\n",
    "- Initialize and fit a k-means clustering model using sklearn's Kmeans() function.\n",
    "- Use the user-defined number of clusters, init='random', n_init=10, random_state=123, and algorithm='elkan'.\n",
    "- Find the cluster centroids and inertia.\n",
    "\n",
    "Ex: If the input is: 4\n",
    "\n",
    "the output should be:\n",
    "\n",
    "- Centroids: [[ 0.8294  0.2562]\n",
    " [ 1.3106 -1.887 ]\n",
    " [-0.9471  0.8281]\n",
    " [-0.6372 -0.7943]]\n",
    "- Inertia: 16.4991"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad54189b-81b1-4a58-9dd8-42ca5ba05310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "healthy = pd.read_csv('healthy_lifestyle.csv')\n",
    "\n",
    "# Input the number of clusters\n",
    "number = int(input())\n",
    "\n",
    "# Define input features\n",
    "X = healthy[['sunshine_hours', 'happiness_levels']]\n",
    "\n",
    "# Drop rows with any NaN values\n",
    "X = X.dropna()\n",
    "\n",
    "# Use StandardScaler() to standardize input features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Initialize a k-means clustering algorithm with k-means++ initialization\n",
    "kmeans = KMeans(init='k-means++', n_clusters=number, n_init=10, random_state=123)\n",
    "\n",
    "# Fit the algorithm to the input features\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Print the cluster centroids and inertia\n",
    "centroids = kmeans.cluster_centers_\n",
    "print(\"Centroids:\", np.round(centroids, 4))\n",
    "\n",
    "inertia = kmeans.inertia_\n",
    "print(\"Inertia:\", np.round(inertia, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18700cd6-8069-458a-8822-61f755fd893e",
   "metadata": {},
   "source": [
    "## 2. Hierarchical clustering using scikit-learn\n",
    "The healthy_lifestyle dataset contains information on lifestyle measures such as amount of sunshine, pollution, and happiness levels for 44 major cities around the world. Apply agglomerative clustering to the cities' number of hours of sunshine and happiness levels using both sklearn and SciPy.\n",
    "\n",
    "- Import the needed packages for agglomerative clustering from sklearn and SciPy.\n",
    "- Initialize and fit an agglomerative clustering model using sklearn's AgglomerativeClustering() function. Use the user-defined number of clusters and ward linkage.\n",
    "- Add cluster labels to the input feature dataframe.\n",
    "- Calculate the distances between all instances using SciPy's pdist() function.\n",
    "- Convert the distance matrix to a square matrix using SciPy's squareform() function.\n",
    "- Define a clustering model with ward linkage using SciPy's linkage() function.\n",
    "\n",
    "Ex: If the input is: 4\n",
    "\n",
    "the output should be:\n",
    "|       | sunshine_hours | happiness_levels | labels |\n",
    "|-------|----------------|------------------|--------|\n",
    "| 0     | -0.691660      | 1.025642         | 3      |\n",
    "| 1     | 0.695725       | 0.801124         | 0      |\n",
    "| 2     | -0.645295      | 0.872562         | 3      |\n",
    "| 3     | -0.757641      | 0.933794         | 3      |\n",
    "| 4     | -1.098246      | 1.229750         | 3      |\n",
    "\n",
    "\n",
    "First five rows of the linkage matrix from SciPy:\n",
    "    \n",
    " - [[39. 40.  0.  2.]\n",
    " [28. 43.  0.  3.]\n",
    " [ 7. 18.  0.  2.]\n",
    " [ 8. 42.  0.  2.]\n",
    " [ 0.  3.  0.  2.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58d1471-d33e-43b5-839c-ba86c9509ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed packages\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "healthy = pd.read_csv('healthy_lifestyle.csv')\n",
    "\n",
    "# Input the number of clusters\n",
    "number = int(input())\n",
    "\n",
    "# Define input features\n",
    "X = healthy[['sunshine_hours', 'happiness_levels']]\n",
    "\n",
    "# Drop rows with any NaN values\n",
    "X = X.dropna()\n",
    "\n",
    "# Use StandardScaler() to standardize input features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=['sunshine_hours', 'happiness_levels'])\n",
    "\n",
    "# Initialize and fit an agglomerative clustering model using ward linkage\n",
    "agglo_clustering = AgglomerativeClustering(n_clusters=number, linkage='ward')\n",
    "X_scaled['labels'] = agglo_clustering.fit_predict(X_scaled)\n",
    "\n",
    "# Round to match example output format\n",
    "X_scaled['sunshine_hours'] = X_scaled['sunshine_hours'].round(6)\n",
    "X_scaled['happiness_levels'] = X_scaled['happiness_levels'].round(6)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(X_scaled.head())\n",
    "\n",
    "# Calculate the distances between all instances\n",
    "distances = pdist(X_scaled[['sunshine_hours', 'happiness_levels']])\n",
    "\n",
    "# Convert the distance matrix to a square matrix\n",
    "square_distances = squareform(distances)\n",
    "\n",
    "# Define a clustering model with ward linkage\n",
    "clustersHealthyScipy = linkage(square_distances, method='ward')\n",
    "print(\"First five rows of the linkage matrix from SciPy:\\n\", np.round(clustersHealthyScipy[:5, :], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db51dd-7ad3-4f58-b0ff-ae71a6c306e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d92b04d3-05bd-470c-a199-327a6b55679c",
   "metadata": {},
   "source": [
    "## 3. DBSCAN using scikit-learn\n",
    "- Increase the **number of points sampled to 500**.\n",
    "- Apply the DBSCAN model with **epsilon=1** and **min_samples=8** to identify the number of core-points and outliers (or noise). \n",
    "- EX: if the epsilon=1 and min_samples = 10 and number of points sampled to 100.\n",
    "  - the number of core-points = 85\n",
    "  - the number of outliers    = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeae7b7-d4e7-483a-81bc-1d4bd4d6716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Load dataset and take a random sample of 500 instances\n",
    "data = pd.read_csv('customer_personality.csv').sample(500, random_state=123)\n",
    "\n",
    "# Use StandardScaler() to standardize input features\n",
    "X = data[['Fruits', 'Meats']]\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X)\n",
    "\n",
    "# Apply DBSCAN with epsilon=1 and min_samples=8\n",
    "dbscan = DBSCAN(eps=1, min_samples=8)\n",
    "dbscan.fit(X)\n",
    "\n",
    "# Print the cluster labels and core point indices\n",
    "print('Labels:', dbscan.labels_)\n",
    "print('Core points:', len(dbscan.core_sample_indices_))\n",
    "print('Number of core points:', len(dbscan.core_sample_indices_))\n",
    "\n",
    "# Add the cluster labels to the dataset as strings\n",
    "data['clusters'] = dbscan.labels_.astype(str)\n",
    "\n",
    "# Sort by cluster label for plotting purposes\n",
    "data.sort_values(by='clusters', inplace=True)\n",
    "\n",
    "# Plot clusters on the original data\n",
    "p = sns.scatterplot(data=data, x='Fruits', y='Meats', hue='clusters', style='clusters')\n",
    "p.set_xlabel('Fruits', fontsize=16)\n",
    "p.set_ylabel('Meats', fontsize=16)\n",
    "p.legend(title='DBSCAN')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea545b6-c934-42ca-9f5a-be7bf69d22d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
